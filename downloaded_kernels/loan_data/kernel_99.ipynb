{"cells":[{"metadata":{"_cell_guid":"60cdff95-e165-4ef7-82bb-be2d3d799248","_uuid":"3fd8e11c1ff348d6b1579a18fe737cec265e04aa"},"cell_type":"markdown","source":"<h1 align=\"center\"> Lending Club Loan Analysis </h1> <br>\n## Company Information:\nLending Club is a  peer to peer lending company based in the United States, in which investors provide funds for potential borrowers and investors earn a profit depending on the risk they take (the borrowers credit score). Lending Club provides the \"bridge\" between investors and borrowers. For more basic information about the company please check out the wikipedia article about the company. <br><br>\n\n\n<a src=\"https://en.wikipedia.org/wiki/Lending_Club\"> Lending Club Information </a>\n\n\n\n\n## How Lending Club Works?\n<img src=\"http://echeck.org/wp-content/uploads/2016/12/Showing-how-the-lending-club-works-and-makes-money-1.png\"><br><br>\n\n\n## Outline: <br><br>\nI. Introduction <br>\na) [General Information](#general_information)<br>\nb) [Similar Distributions](#similar_distributions)<br><br>\n\nII. <b>Good Loans vs Bad Loans</b><br>\na) [Types of Loans](#types_of_loans)<br>\nb) [Loans issued by Region](#by_region)<br>\nc) [A Deeper Look into Bad Loans](#deeper_bad_loans)<br><br>\n\nIII. <b>The Business Perspective</b><br>\na) [Understanding the Operative side of Business](#operative_side)<br>\nb) [Analysis by Income Category](#income_category) <br><br>\n\nIV. <b>Assesing Risks</b><br>\na) [Understanding the Risky Side of Business](#risky_side)<br>\nb) [The importance of Credit Scores](#credit_scores)<br>\nc) [What determines a bad loan](#determines_bad_loan)<br>\nd) [Defaulted Loans](#defaulted_loans)\n\n## References:\n1) <a src=\"https://www.kaggle.com/arthurtok/global-religion-1945-2010-plotly-pandas-visuals\"> Global Religion 1945-2010: Plotly & Pandas visuals</a> by Anisotropic <br>\n2) <a src=\"https://www.kaggle.com/vigilanf/loan-metrics-by-state\"> Loan Metrics By State </a> by Frank Vigilante<br>\n3) Hands on Machine Learning by Aurélien Géron <br>\n4) <a src=\"https://www.youtube.com/watch?v=oYbVFhK_olY&list=PLSPWNkAMSvv5DKeSVDbEbUKSsK4Z-GgiP\"> Deep Learning with Neural Networks and TensorFlow </a> by Sentdex"},{"metadata":{"_cell_guid":"08fbe334-0d76-4ba1-9cb5-f0dc62d61dce","_uuid":"50b0ff766333a9036729ff6c0e36ca9cafe4df06"},"cell_type":"markdown","source":"# Introduction:\n## General Information:\n<a id=\"general_information\"></a>"},{"metadata":{"_cell_guid":"428e7e85-93ed-45b1-aa24-cba4a771c2f3","_uuid":"d65b7b14701cf9c42ec8cbb1ecfdfe6ab3358ae7","trusted":true},"cell_type":"code","source":"# Import our libraries we are going to use for our data analysis.\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Plotly visualizations\nfrom plotly import tools\nimport plotly.plotly as py\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n# plotly.tools.set_credentials_file(username='AlexanderBach', api_key='o4fx6i1MtEIJQxfWYvU1')\n\n\n% matplotlib inline\n\ndf = pd.read_csv('../input/loan.csv', low_memory=False)\n\n# Copy of the dataframe\noriginal_df = df.copy()\n\ndf.head()","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"7e81133a-2dbe-4126-8ad6-e1c84efa915f","_uuid":"0ee2926e3668a0c3beb7b06ce9f14809c1e37053","trusted":true},"cell_type":"code","source":"df.info()","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"f054ecf3-735e-4ff8-a701-9210507e1cac","_uuid":"0ff8dc19af3f2ad57e2c9c8c02a2047fb502bd1e","collapsed":true,"trusted":true},"cell_type":"code","source":"# Replace the name of some columns\ndf = df.rename(columns={\"loan_amnt\": \"loan_amount\", \"funded_amnt\": \"funded_amount\", \"funded_amnt_inv\": \"investor_funds\",\n                       \"int_rate\": \"interest_rate\", \"annual_inc\": \"annual_income\"})\n\n# Drop irrelevant columns\ndf.drop(['id', 'member_id', 'emp_title', 'url', 'desc', 'zip_code', 'title'], axis=1, inplace=True)\n","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"9390f158-796f-4f69-b145-b1d2a5584745","_uuid":"ab357844a9985a3783a813423e08f5166830203b"},"cell_type":"markdown","source":"## Similar Distributions:\n<a id=\"similar_distributions\"></a>\nWe will start by exploring the distribution of the loan amounts and see when did the loan amount issued increased significantly. <br>\n\n<h4> What we need to know: </h4> <br>\n<ul>\n<li> Understand what amount was <b>mostly issued</b> to borrowers. </li>\n<li> Which <b>year</b> issued the most loans. </li>\n<li> The distribution of loan amounts is a <b>multinomial distribution </b>.</li>\n</ul>\n\n\n\n<h4> Summary: </h4><br>\n<ul>\n<li> Most of the <b>loans issued</b> were in the range of 10,000 to 20,000 USD. </li>\n<li> The <b>year of 2015</b> was the year were most loans were issued.</li> \n<li> Loans were issued in an <b>incremental manner</b>. (Possible due to a recovery in the U.S economy) </li>\n<li> The loans <b>applied</b> by potential borrowers, the amount <b>issued</b> to the borrowers and the amount <b>funded</b> by investors are similarly distributed, <b>meaning</b> that it is most likely that qualified borrowers are going to get the loan they had applied for. </li>\n\n</ul>\n\n\n\n"},{"metadata":{"_cell_guid":"fe7b0217-b372-444e-9f6c-d1ed5fe5259c","_uuid":"dca1e813e0820c7e422c4eadf66e2a71fda28b94","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 3, figsize=(16,5))\n\n\n\n\nloan_amount = df[\"loan_amount\"].values\nfunded_amount = df[\"funded_amount\"].values\ninvestor_funds = df[\"investor_funds\"].values\n\n\nsns.distplot(loan_amount, ax=ax[0], color=\"#F7522F\")\nax[0].set_title(\"Loan Applied by the Borrower\", fontsize=14)\nsns.distplot(funded_amount, ax=ax[1], color=\"#2F8FF7\")\nax[1].set_title(\"Amount Funded by the Lender\", fontsize=14)\nsns.distplot(investor_funds, ax=ax[2], color=\"#2EAD46\")\nax[2].set_title(\"Total committed by Investors\", fontsize=14)","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"b0d59461-2e0f-44e7-bb9c-7dc57934b488","_uuid":"de70a0eb750a61d68838339b3bf60f9136585c29","collapsed":true,"trusted":true},"cell_type":"code","source":"# Lets' transform the issue dates by year.\ndf['issue_d'].head()\ndt_series = pd.to_datetime(df['issue_d'])\ndf['year'] = dt_series.dt.year","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"61bff410-2904-4f17-93e5-c724a778e019","_uuid":"04a7f159ba7620fd10780e7ff9c1320f428f83b5","trusted":true},"cell_type":"code","source":"# The year of 2015 was the year were the highest amount of loans were issued \n# This is an indication that the economy is quiet recovering itself.\nplt.figure(figsize=(12,8))\nsns.barplot('year', 'loan_amount', data=df, palette='tab10')\nplt.title('Issuance of Loans', fontsize=16)\nplt.xlabel('Year', fontsize=14)\nplt.ylabel('Average loan amount issued', fontsize=14)","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"68557789-fb4c-43d9-a5c5-f588085aace7","_uuid":"cc873fd882cba9ee89a01f64ce9b390fdc3b9f25"},"cell_type":"markdown","source":"<h1 align=\"center\"> Good Loans vs Bad Loans: </h1>\n<h2>Types of Loans: </h2>\n<a id=\"types_of_loans\"></a>\n<img src=\"http://strongarticle.com/wp-content/uploads/2017/09/1f42d6e77042d87f3bb6ae171ebbc530.jpg\">\n<br><br>\nIn this section, we will see what is the amount of bad loans Lending Club has declared so far, of course we have to understand that there are still loans that are at a risk of defaulting in the future. \n\n<h4> What we need to know: </h4>\n<ul>\n<li> The amount of bad loans could <b>increment</b> as the days pass by, since we still have a great amount of current loans. </li>\n<li> <b>Average annual income</b> is an important key metric for finding possible opportunities of investments in a specific region. </li>\n\n</ul>\n\n<h4> Summary: </h4>\n<ul>\n<li> Currently, <b>bad loans</b> consist 7.60% of total loans but remember that we still have <b>current loans</b> which have the risk of becoming bad loans. (So this percentage is subjected to possible changes.) </li>\n<li> The <b> NorthEast </b> region seems to be the most attractive in term of funding loans to borrowers. </li>\n<li> The <b> SouthWest </b> and <b> West</b> regions have experienced a slight increase in the \"median income\" in the past years. </li> \n<li> <b>Average interest</b> rates have declined since 2012 but this might explain the <b>increase in the volume</b> of loans.  </li>\n<li> <b>Employment Length</b> tends to be greater in the regions of the <b>SouthWest</b> and <b>West</b></li>\n<li> Clients located in the regions of <b>NorthEast</b> and <b>MidWest</b> have not experienced a drastic increase in debt-to-income(dti) as compared to the other regions. </li>\n</ul>"},{"metadata":{"_cell_guid":"85412154-d30d-4f7f-96b2-d41b6d94ab74","_uuid":"f6cd906337ab70e068887b011bd29dd1e628b0f1","trusted":true},"cell_type":"code","source":"df[\"loan_status\"].value_counts()","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"61d96d6a-f3f8-4a34-a4c8-3d77c4ebee31","_uuid":"6fd3553700f3eb4889e88517d2b8e5b7d904872e","collapsed":true,"trusted":true},"cell_type":"code","source":"# Determining the loans that are bad from loan_status column\n\nbad_loan = [\"Charged Off\", \"Default\", \"Does not meet the credit policy. Status:Charged Off\", \"In Grace Period\", \n            \"Late (16-30 days)\", \"Late (31-120 days)\"]\n\n\ndf['loan_condition'] = np.nan\n\ndef loan_condition(status):\n    if status in bad_loan:\n        return 'Bad Loan'\n    else:\n        return 'Good Loan'\n    \n    \ndf['loan_condition'] = df['loan_status'].apply(loan_condition)","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"ac878d77-5702-4daf-adfe-d61657043e90","_uuid":"596bd11ee859fec86ca3a4469b6f98a7f13f4035","trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1,2, figsize=(16,8))\n\ncolors = [\"#3791D7\", \"#D72626\"]\nlabels =\"Good Loans\", \"Bad Loans\"\n\nplt.suptitle('Information on Loan Conditions', fontsize=20)\n\ndf[\"loan_condition\"].value_counts().plot.pie(explode=[0,0.25], autopct='%1.2f%%', ax=ax[0], shadow=True, colors=colors, \n                                             labels=labels, fontsize=12, startangle=70)\n\n\n# ax[0].set_title('State of Loan', fontsize=16)\nax[0].set_ylabel('% of Condition of Loans', fontsize=14)\n\n# sns.countplot('loan_condition', data=df, ax=ax[1], palette=colors)\n# ax[1].set_title('Condition of Loans', fontsize=20)\n# ax[1].set_xticklabels(['Good', 'Bad'], rotation='horizontal')\npalette = [\"#3791D7\", \"#E01E1B\"]\n\nsns.barplot(x=\"year\", y=\"loan_amount\", hue=\"loan_condition\", data=df, palette=palette, estimator=lambda x: len(x) / len(df) * 100)\nax[1].set(ylabel=\"(%)\")","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"6871315f-0d27-4b08-8502-247fb711ec50","_uuid":"cef68413efdb6b4f48b0c640bd0c5e877800ede1"},"cell_type":"markdown","source":"<h2> Loans Issued by Region</h2>\n<a id=\"by_region\"></a>\nIn this section we want to analyze loans issued by region in order to see region patters that will allow us to understand which region gives Lending Club.<br><br>\n\n## Summary: <br>\n<ul>\n<li> <b> SouthEast</b> , <b>West </b> and <b>NorthEast</b> regions had the highest amount lof loans issued. </li>\n<li> <b>West </b> and <b>SouthWest </b> had a rapid increase in debt-to-income starting in 2012. </li>\n<li><b>West </b> and <b>SouthWest </b>  had a rapid decrease in interest rates (This might explain the increase in debt to income). </li>\n</ul>"},{"metadata":{"_cell_guid":"f82148c1-5a13-4af0-b7b3-c6fb767864ca","_uuid":"3af220b72bb3bfd067086f3c2049b6e04da0808c","collapsed":true,"trusted":true},"cell_type":"code","source":"df['addr_state'].unique()\n\n# Make a list with each of the regions by state.\n\nwest = ['CA', 'OR', 'UT','WA', 'CO', 'NV', 'AK', 'MT', 'HI', 'WY', 'ID']\nsouth_west = ['AZ', 'TX', 'NM', 'OK']\nsouth_east = ['GA', 'NC', 'VA', 'FL', 'KY', 'SC', 'LA', 'AL', 'WV', 'DC', 'AR', 'DE', 'MS', 'TN' ]\nmid_west = ['IL', 'MO', 'MN', 'OH', 'WI', 'KS', 'MI', 'SD', 'IA', 'NE', 'IN', 'ND']\nnorth_east = ['CT', 'NY', 'PA', 'NJ', 'RI','MA', 'MD', 'VT', 'NH', 'ME']\n\n\n\ndf['region'] = np.nan\n\ndef finding_regions(state):\n    if state in west:\n        return 'West'\n    elif state in south_west:\n        return 'SouthWest'\n    elif state in south_east:\n        return 'SouthEast'\n    elif state in mid_west:\n        return 'MidWest'\n    elif state in north_east:\n        return 'NorthEast'\n    \n\n\ndf['region'] = df['addr_state'].apply(finding_regions)","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"74afbde0-508f-4bfb-92de-373c3f6dd0e2","_uuid":"03f1590c7a754ccba10fd3a7051a587f4a4b98da","collapsed":true,"trusted":true},"cell_type":"code","source":"# This code will take the current date and transform it into a year-month format\ndf['complete_date'] = pd.to_datetime(df['issue_d'])\n\ngroup_dates = df.groupby(['complete_date', 'region'], as_index=False).sum()\n\ngroup_dates['issue_d'] = [month.to_period('M') for \n                          month in group_dates['complete_date']]\n\ngroup_dates = group_dates.groupby(['issue_d', 'region'], as_index=False).sum()\ngroup_dates = group_dates.groupby(['issue_d', 'region'], as_index=False).sum()\ngroup_dates['loan_amount'] = group_dates['loan_amount']/1000\n\n\ndf_dates = pd.DataFrame(data=group_dates[['issue_d','region','loan_amount']])","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"9de5aa56-be65-49bc-8c02-a12eca0f8aa0","_uuid":"a1c7340953b00c99b113e915a39bfc6457fc4981","trusted":true},"cell_type":"code","source":"plt.style.use('dark_background')\ncmap = plt.cm.Set3\n\nby_issued_amount = df_dates.groupby(['issue_d', 'region']).loan_amount.sum()\nby_issued_amount.unstack().plot(stacked=False, colormap=cmap, grid=False, legend=True, figsize=(15,6))\n\nplt.title('Loans issued by Region', fontsize=16)","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"5fe5597b-abd4-4366-88b5-7c735692b0c2","_uuid":"b26ef6d24ea7d2dc41fe2a2626694399e61b6141","collapsed":true,"trusted":true},"cell_type":"code","source":"employment_length = ['10+ years', '< 1 year', '1 year', '3 years', '8 years', '9 years',\n                    '4 years', '5 years', '6 years', '2 years', '7 years', 'n/a']\n\n# Create a new column and convert emp_length to integers.\n\nlst = [df]\ndf['emp_length_int'] = np.nan\n\nfor col in lst:\n    col.loc[col['emp_length'] == '10+ years', \"emp_length_int\"] = 10\n    col.loc[col['emp_length'] == '9 years', \"emp_length_int\"] = 9\n    col.loc[col['emp_length'] == '8 years', \"emp_length_int\"] = 8\n    col.loc[col['emp_length'] == '7 years', \"emp_length_int\"] = 7\n    col.loc[col['emp_length'] == '6 years', \"emp_length_int\"] = 6\n    col.loc[col['emp_length'] == '5 years', \"emp_length_int\"] = 5\n    col.loc[col['emp_length'] == '4 years', \"emp_length_int\"] = 4\n    col.loc[col['emp_length'] == '3 years', \"emp_length_int\"] = 3\n    col.loc[col['emp_length'] == '2 years', \"emp_length_int\"] = 2\n    col.loc[col['emp_length'] == '1 year', \"emp_length_int\"] = 1\n    col.loc[col['emp_length'] == '< 1 year', \"emp_length_int\"] = 0.5\n    col.loc[col['emp_length'] == 'n/a', \"emp_length_int\"] = 0\n    \n    \n","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"8c453a61-ce53-459b-bf38-d02a56d24b81","_uuid":"fc10ef7575055551e8bce935b642c0fcceb93527","trusted":true},"cell_type":"code","source":"# Loan issued by Region and by Credit Score grade\n# Change the colormap for tomorrow!\n\nsns.set_style('whitegrid')\n\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\ncmap = plt.cm.inferno\n\nby_interest_rate = df.groupby(['year', 'region']).interest_rate.mean()\nby_interest_rate.unstack().plot(kind='area', stacked=True, colormap=cmap, grid=False, legend=False, ax=ax1, figsize=(16,12))\nax1.set_title('Average Interest Rate by Region', fontsize=14)\n\n\nby_employment_length = df.groupby(['year', 'region']).emp_length_int.mean()\nby_employment_length.unstack().plot(kind='area', stacked=True, colormap=cmap, grid=False, legend=False, ax=ax2, figsize=(16,12))\nax2.set_title('Average Employment Length by Region', fontsize=14)\n# plt.xlabel('Year of Issuance', fontsize=14)\n\nby_dti = df.groupby(['year', 'region']).dti.mean()\nby_dti.unstack().plot(kind='area', stacked=True, colormap=cmap, grid=False, legend=False, ax=ax3, figsize=(16,12))\nax3.set_title('Average Debt-to-Income by Region', fontsize=14)\n\nby_income = df.groupby(['year', 'region']).annual_income.mean()\nby_income.unstack().plot(kind='area', stacked=True, colormap=cmap, grid=False, ax=ax4, figsize=(16,12))\nax4.set_title('Average Annual Income by Region', fontsize=14)\nax4.legend(bbox_to_anchor=(-1.0, -0.5, 1.8, 0.1), loc=10,prop={'size':12},\n           ncol=5, mode=\"expand\", borderaxespad=0.)","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"ec3c1e24-70e6-45bb-97ea-4c42ff69f84d","_uuid":"234064b3f172cd733f7f416149b663b37ab4268b"},"cell_type":"markdown","source":"## A Deeper Look into Bad Loans:\n<a id=\"deeper_bad_loans\"></a>\n\n<h4> What we need to know: </h4>\n<ul> \n<li>The number of loans that were classified as bad loans for each region by its <b>loan status</b>. (This will be shown in a dataframe below.)</li>\n<li> This won't give us the exact reasons why a loan is categorized as a bad loan (other variables that might have influence the condition of the loan) but it will give us a <b> deeper insight on the level of risk </b> in a particular region. </li>\n</ul>\n\n<h4> Summary: </h4>\n<ul>\n<li>The regions of the <b> West </b> and <b> SouthEast </b> had a higher percentage in most of the b \"bad\" loan statuses.</li>\n<li> The <b>NorthEast</b> region had a higher percentage in <b>Grace Period</b> and <b>Does not meet Credit Policy</b> loan status. However, both of these are not considered as bad as <b>default</b> for instance. </li>\n<li> Based on this small and brief summary we can conclude that the <b>West</b> and <b>SouthEast</b> regions have the most undesirable loan status, but just by a slightly higher percentage compared to the <b>NorthEast</b> region. </li>\n<li> Again, this does not tell us what causes a loan to be a <b> bad loan </b>, but it gives us some idea about <b>the level of risk</b> within the regions across the United States. </li>\n</ul>"},{"metadata":{"_cell_guid":"4ec04af3-a602-4547-982d-f42dd15b2914","_uuid":"b72bf3be29058e6f859939ea2f81a0f226a931e4","trusted":true},"cell_type":"code","source":"# We have 67429 loans categorized as bad loans\nbadloans_df = df.loc[df[\"loan_condition\"] == \"Bad Loan\"]\n\n# loan_status cross\nloan_status_cross = pd.crosstab(badloans_df['region'], badloans_df['loan_status']).apply(lambda x: x/x.sum() * 100)\nnumber_of_loanstatus = pd.crosstab(badloans_df['region'], badloans_df['loan_status'])\n\n\n# Round our values\nloan_status_cross['Charged Off'] = loan_status_cross['Charged Off'].apply(lambda x: round(x, 2))\nloan_status_cross['Default'] = loan_status_cross['Default'].apply(lambda x: round(x, 2))\nloan_status_cross['Does not meet the credit policy. Status:Charged Off'] = loan_status_cross['Does not meet the credit policy. Status:Charged Off'].apply(lambda x: round(x, 2))\nloan_status_cross['In Grace Period'] = loan_status_cross['In Grace Period'].apply(lambda x: round(x, 2))\nloan_status_cross['Late (16-30 days)'] = loan_status_cross['Late (16-30 days)'].apply(lambda x: round(x, 2))\nloan_status_cross['Late (31-120 days)'] = loan_status_cross['Late (31-120 days)'].apply(lambda x: round(x, 2))\n\n\nnumber_of_loanstatus['Total'] = number_of_loanstatus.sum(axis=1) \n# number_of_badloans\nnumber_of_loanstatus","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"2924ac56-0368-45e1-8474-3d3ffb78651c","_uuid":"f18e32fe594a649502c501c296dc0ea75bd709d3","trusted":true},"cell_type":"code","source":"charged_off = loan_status_cross['Charged Off'].values.tolist()\ndefault = loan_status_cross['Default'].values.tolist()\nnot_meet_credit = loan_status_cross['Does not meet the credit policy. Status:Charged Off'].values.tolist()\ngrace_period = loan_status_cross['In Grace Period'].values.tolist()\nshort_pay = loan_status_cross['Late (16-30 days)'] .values.tolist()\nlong_pay = loan_status_cross['Late (31-120 days)'].values.tolist()\n\n\n\ncharged = go.Bar(\n    x=['MidWest', 'NorthEast', 'SouthEast', 'SouthWest', 'West'],\n    y= charged_off,\n    name='Charged Off',\n    marker=dict(\n        color='rgb(192, 148, 246)'\n    ),\n    text = '%'\n)\n\ndefaults = go.Bar(\n    x=['MidWest', 'NorthEast', 'SouthEast', 'SouthWest', 'West'],\n    y=default,\n    name='Defaults',\n    marker=dict(\n        color='rgb(176, 26, 26)'\n    ),\n    text = '%'\n)\n\ncredit_policy = go.Bar(\n    x=['MidWest', 'NorthEast', 'SouthEast', 'SouthWest', 'West'],\n    y= not_meet_credit,\n    name='Does not meet Credit Policy',\n    marker = dict(\n        color='rgb(229, 121, 36)'\n    ),\n    text = '%'\n)\n\ngrace = go.Bar(\n    x=['MidWest', 'NorthEast', 'SouthEast', 'SouthWest', 'West'],\n    y= grace_period,\n    name='Grace Period',\n    marker = dict(\n        color='rgb(147, 147, 147)'\n    ),\n    text = '%'\n)\n\nshort_pays = go.Bar(\n    x=['MidWest', 'NorthEast', 'SouthEast', 'SouthWest', 'West'],\n    y= short_pay,\n    name='Late Payment (16-30 days)', \n    marker = dict(\n        color='rgb(246, 157, 135)'\n    ),\n    text = '%'\n)\n\nlong_pays = go.Bar(\n    x=['MidWest', 'NorthEast', 'SouthEast', 'SouthWest', 'West'],\n    y= long_pay,\n    name='Late Payment (31-120 days)',\n    marker = dict(\n        color = 'rgb(238, 76, 73)'\n        ),\n    text = '%'\n)\n\n\n\n\ndata = [charged, defaults, credit_policy, grace, short_pays, long_pays]\nlayout = go.Layout(\n    barmode='stack',\n    title = '% of Bad Loan Status by Region',\n    xaxis=dict(title='US Regions')\n)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='stacked-bar')","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"5da6bde7-19fb-475c-a594-125288851fb5","_uuid":"66fe044e83e0695eda2817ee8e02e8ed6e03fccb","trusted":true},"cell_type":"code","source":"# Average interest rates clients pay\ndf['interest_rate'].mean()\n# Average annual income of clients\ndf['annual_income'].mean()","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"22cce5bc-2b49-4c3a-9966-3d9b0e07c250","_uuid":"138a17bf8f796d021221755065c174401cdff6c2"},"cell_type":"markdown","source":"<h1 align=\"center\"> The Business Perspective </h1>\n<h2 > Understanding the Operative Side of Business </h2>\n<a id=\"operative_side\"></a>\n<img src=\"http://bestcredit.sg/wp-content/uploads/2017/07/licensed-money-lender.jpg\"><br><br>\nNow we will have a closer look at the <b> operative side </b> of business by state. This will give us a clearer idea in which state we have a higher operating activity. This will allow us to ask further questions such as Why do we have a higher level of operating activity in this state? Could it be because of economic factors? or the risk level is low and returns are fairly decent? Let's explore!\n\n<h4> What we need to know: </h4>\n<ul>\n<li> We will focus on <b>three key metrics</b>: Loans issued by state (Total Sum), Average interest rates charged to customers and average annual income of all customers by state. </li>\n<li> The purpose of this analysis is to see states that give high returns at a descent risk. </li>\n\n</ul>\n\n<h4> Summary: </h4>\n<ul>\n<li> <b>California, Texas, New York and Florida</b> are the states in which the highest amount of loans were issued. </li>\n<li> Interesting enough, all four states have a approximate <b>interest rate of 13%</b> which is at the same level of the average interest rate for all states (13.24%) </li>\n<li> California, Texas and New York are <b>all above the average annual income</b> (with the exclusion of Florida), this might give possible indication why most loans are issued in these states. </li>\n</ul>"},{"metadata":{"_cell_guid":"23bcd908-a7fe-455e-a658-59b6fd9b7d64","_uuid":"2d6663f515fcb697b6b0bc4bf138734e3d6beae9","trusted":true},"cell_type":"code","source":"# Plotting by states\n\n# Grouping by our metrics\n# First Plotly Graph (We evaluate the operative side of the business)\nby_loan_amount = df.groupby(['region','addr_state'], as_index=False).loan_amount.sum()\nby_interest_rate = df.groupby(['region', 'addr_state'], as_index=False).interest_rate.mean()\nby_income = df.groupby(['region', 'addr_state'], as_index=False).annual_income.mean()\n\n\n\n# Take the values to a list for visualization purposes.\nstates = by_loan_amount['addr_state'].values.tolist()\naverage_loan_amounts = by_loan_amount['loan_amount'].values.tolist()\naverage_interest_rates = by_interest_rate['interest_rate'].values.tolist()\naverage_annual_income = by_income['annual_income'].values.tolist()\n\n\nfrom collections import OrderedDict\n\n# Figure Number 1 (Perspective for the Business Operations)\nmetrics_data = OrderedDict([('state_codes', states),\n                            ('issued_loans', average_loan_amounts),\n                            ('interest_rate', average_interest_rates),\n                            ('annual_income', average_annual_income)])\n                     \n\nmetrics_df = pd.DataFrame.from_dict(metrics_data)\nmetrics_df = metrics_df.round(decimals=2)\nmetrics_df.head()\n\n\n\n# Think of a way to add default rate\n# Consider adding a few more metrics for the future","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"9658d3df-f822-4d89-ae55-cb1e2559507b","_uuid":"d034ebaaa95dd36353bb96665bcca7117a56a7c5","trusted":true},"cell_type":"code","source":"# Now it comes the part where we plot out plotly United States map\nimport plotly.plotly as py\nimport plotly.graph_objs as go\n\n\nfor col in metrics_df.columns:\n    metrics_df[col] = metrics_df[col].astype(str)\n    \nscl = [[0.0, 'rgb(210, 241, 198)'],[0.2, 'rgb(188, 236, 169)'],[0.4, 'rgb(171, 235, 145)'],\\\n            [0.6, 'rgb(140, 227, 105)'],[0.8, 'rgb(105, 201, 67)'],[1.0, 'rgb(59, 159, 19)']]\n\nmetrics_df['text'] = metrics_df['state_codes'] + '<br>' +\\\n'Average loan interest rate: ' + metrics_df['interest_rate'] + '<br>'+\\\n'Average annual income: ' + metrics_df['annual_income'] \n\n\ndata = [ dict(\n        type='choropleth',\n        colorscale = scl,\n        autocolorscale = False,\n        locations = metrics_df['state_codes'],\n        z = metrics_df['issued_loans'], \n        locationmode = 'USA-states',\n        text = metrics_df['text'],\n        marker = dict(\n            line = dict (\n                color = 'rgb(255,255,255)',\n                width = 2\n            ) ),\n        colorbar = dict(\n            title = \"$s USD\")\n        ) ]\n\n\nlayout = dict(\n    title = 'Lending Clubs Issued Loans <br> (A Perspective for the Business Operations)',\n    geo = dict(\n        scope = 'usa',\n        projection=dict(type='albers usa'),\n        showlakes = True,\n        lakecolor = 'rgb(255, 255, 255)')\n)\n\nfig = dict(data=data, layout=layout)\niplot(fig, filename='d3-cloropleth-map')","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"70d317a6-1c8f-497e-95b0-484931a6505e","_uuid":"afa20ec1ac2f9c7170e0bf5def350833db1526ff"},"cell_type":"markdown","source":"## Analysis by Income Category:\n<a id=\"income_category\"></a>\nIn this section we will create different <b> income categories </b> in order to detect important patters and go more into depth in our analysis.\n\n**What we need to know:** <br>\n<ul>\n<li><b>Low income category:</b> Borrowers that have an annual income lower or equal to 100,000 usd.</li>\n<li> <b> Medium income category:</b> Borrowers that have an annual income higher than 100,000 usd but lower or equal to 200,000 usd. </li>\n<li><b> High income category: </b> Borrowers that have an annual income higher tha 200,000 usd. </li>\n</ul>\n\n**Summary:**\n<ul>\n<li>Borrowers that made part of the <b>high income category</b> took higher loan amounts than people from <b>low</b> and <b>medium income categories.</b> Of course, people with higher annual incomes are more likely to pay loans with a higher amount. (First row to the left of the subplots) </li>\n<li> Loans that were borrowed by the <b>Low income category</b> had a slightly higher change of becoming a bad loan. (First row to the right of the subplots) </li>\n<li>Borrowers with <b>High</b> and <b> Medium</b> annual incomes had a longer employment length than people with lower incomes.(Second row to the left of the subplots) </li>\n<li> Borrowers with a lower income had on average <b>higher interest rates</b> while people with a higher annual income had <b>lower interest rates</b> on their loans. (Second row to the right of the subplots)</li> \n\n</ul>"},{"metadata":{"_cell_guid":"9703d5e9-d59b-4ed3-8a7c-ffb2eb118482","_uuid":"91adcc85d6847933a074c9687c16755d1cb2b4d5","collapsed":true,"trusted":true},"cell_type":"code","source":"# Let's create categories for annual_income since most of the bad loans are located below 100k\n\ndf['income_category'] = np.nan\nlst = [df]\n\nfor col in lst:\n    col.loc[col['annual_income'] <= 100000, 'income_category'] = 'Low'\n    col.loc[(col['annual_income'] > 100000) & (col['annual_income'] <= 200000), 'income_category'] = 'Medium'\n    col.loc[col['annual_income'] > 200000, 'income_category'] = 'High'","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"697206b1-4a51-4b71-816c-f683e1e1e1b3","_uuid":"306437009da2006a4253e97bdd09b8b6f3f5b2d1","collapsed":true,"trusted":true},"cell_type":"code","source":"# Let's transform the column loan_condition into integrers.\n\nlst = [df]\ndf['loan_condition_int'] = np.nan\n\nfor col in lst:\n    col.loc[df['loan_condition'] == 'Bad Loan', 'loan_condition_int'] = 0 # Negative (Bad Loan)\n    col.loc[df['loan_condition'] == 'Good Loan', 'loan_condition_int'] = 1 # Positive (Good Loan)","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"ba55fd70-e3aa-4e97-869e-d4af2928f0b1","_uuid":"69859cabf2fb48c3f6efb3e1256035bf3f57f270","trusted":true},"cell_type":"code","source":"fig, ((ax1, ax2), (ax3, ax4))= plt.subplots(nrows=2, ncols=2, figsize=(14,6))\n\n# Change the Palette types tomorrow!\n\nsns.violinplot(x=\"income_category\", y=\"loan_amount\", data=df, palette=\"Set2\", ax=ax1 )\nsns.violinplot(x=\"income_category\", y=\"loan_condition_int\", data=df, palette=\"Set2\", ax=ax2)\nsns.boxplot(x=\"income_category\", y=\"emp_length_int\", data=df, palette=\"Set2\", ax=ax3)\nsns.boxplot(x=\"income_category\", y=\"interest_rate\", data=df, palette=\"Set2\", ax=ax4)","execution_count":22,"outputs":[]},{"metadata":{"_cell_guid":"a5762c82-4b5c-457e-9295-f7732c21d38c","_uuid":"9f4ca83a49dd49ba2cdef23a6f98ef02ca413c12"},"cell_type":"markdown","source":"<h1 align=\"center\"> Assesing Risks </h1>\n<h2> Understanding the Risky side of Business </h2>\n<a id=\"risky_side\"></a>\n\nAlthough the <b> operative side of business </b> is important, we have to also analyze the level of risk in each state. Credit scores are important metrics to analyze the level of risk of an individual customer. However, there are also other important metrics to somehow estimate the level of risk of other states. <br><br>\n\n<h4> What we need to know: </h4>\n<ul>\n<li> <b>Debt-to-income</b> is an important metric since it says approximately the level of debt of each individual consumer with respect to its total income. </li>\n<li> The <b>average length of employment</b> tells us a better story about the labor market in each state which is helpful to assess the levelof risk. </li>\n</ul>\n\n<h4> Summary: </h4>\n<ul>\n<li> <b>IOWA</b> has the highest level of default ratio neverthless, the amount of loans issued in that state is <b>too low</b>. (Number of Bad loans is equal to 3) </li>\n<li> California and Texas seem to have the lowest risk and the highest possible return for investors. However, I will look more deeply into these states and create other metrics analyze the level of risk for each state. </li>\n\n</ul>\n\n\n**Note: I will be updating these section sooner or later (Stay in touch!)**"},{"metadata":{"_cell_guid":"43d99fa7-4658-4463-bb49-74ed4d0e1063","_uuid":"222fe26e9f614a715da22bcdd1f10746d76d35a2","trusted":true},"cell_type":"code","source":"by_condition = df.groupby('addr_state')['loan_condition'].value_counts()/ df.groupby('addr_state')['loan_condition'].count()\nby_emp_length = df.groupby(['region', 'addr_state'], as_index=False).emp_length_int.mean().sort_values(by=\"addr_state\")\n\nloan_condition_bystate = pd.crosstab(df['addr_state'], df['loan_condition'] )\n\ncross_condition = pd.crosstab(df[\"addr_state\"], df[\"loan_condition\"])\n# Percentage of condition of loan\npercentage_loan_contributor = pd.crosstab(df['addr_state'], df['loan_condition']).apply(lambda x: x/x.sum() * 100)\ncondition_ratio = cross_condition[\"Bad Loan\"]/cross_condition[\"Good Loan\"]\nby_dti = df.groupby(['region', 'addr_state'], as_index=False).dti.mean()\nstate_codes = sorted(states)\n\n\n# Take to a list\ndefault_ratio = condition_ratio.values.tolist()\naverage_dti = by_dti['dti'].values.tolist()\naverage_emp_length = by_emp_length[\"emp_length_int\"].values.tolist()\nnumber_of_badloans = loan_condition_bystate['Bad Loan'].values.tolist()\npercentage_ofall_badloans = percentage_loan_contributor['Bad Loan'].values.tolist()\n\n\n# Figure Number 2\nrisk_data = OrderedDict([('state_codes', state_codes),\n                         ('default_ratio', default_ratio),\n                         ('badloans_amount', number_of_badloans),\n                         ('percentage_of_badloans', percentage_ofall_badloans),\n                         ('average_dti', average_dti),\n                         ('average_emp_length', average_emp_length)])\n\n\n# Figure 2 Dataframe \nrisk_df = pd.DataFrame.from_dict(risk_data)\nrisk_df = risk_df.round(decimals=3)\nrisk_df.head()","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"39a3e3fb-c123-4258-8d56-99f48954c4df","_uuid":"2e1e336a86a51b03a20f29776c994b03e4cf5a63","trusted":true},"cell_type":"code","source":"# Now it comes the part where we plot out plotly United States map\nimport plotly.plotly as py\nimport plotly.graph_objs as go\n\n\nfor col in risk_df.columns:\n    risk_df[col] = risk_df[col].astype(str)\n    \nscl = [[0.0, 'rgb(202, 202, 202)'],[0.2, 'rgb(253, 205, 200)'],[0.4, 'rgb(252, 169, 161)'],\\\n            [0.6, 'rgb(247, 121, 108  )'],[0.8, 'rgb(232, 70, 54)'],[1.0, 'rgb(212, 31, 13)']]\n\nrisk_df['text'] = risk_df['state_codes'] + '<br>' +\\\n'Number of Bad Loans: ' + risk_df['badloans_amount'] + '<br>' + \\\n'Percentage of all Bad Loans: ' + risk_df['percentage_of_badloans'] + '%' +  '<br>' + \\\n'Average Debt-to-Income Ratio: ' + risk_df['average_dti'] + '<br>'+\\\n'Average Length of Employment: ' + risk_df['average_emp_length'] \n\n\ndata = [ dict(\n        type='choropleth',\n        colorscale = scl,\n        autocolorscale = False,\n        locations = risk_df['state_codes'],\n        z = risk_df['default_ratio'], \n        locationmode = 'USA-states',\n        text = risk_df['text'],\n        marker = dict(\n            line = dict (\n                color = 'rgb(255,255,255)',\n                width = 2\n            ) ),\n        colorbar = dict(\n            title = \"%\")\n        ) ]\n\n\nlayout = dict(\n    title = 'Lending Clubs Default Rates <br> (Analyzing Risks)',\n    geo = dict(\n        scope = 'usa',\n        projection=dict(type='albers usa'),\n        showlakes = True,\n        lakecolor = 'rgb(255, 255, 255)')\n)\n\nfig = dict(data=data, layout=layout)\niplot(fig, filename='d3-cloropleth-map')","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"cbe5e3e3-289c-45f8-9cb9-e5792363fd53","_uuid":"be1fa255c95aabede8da476fdfedd40da5b3b4a6"},"cell_type":"markdown","source":"## The Importance of Credit Scores:\n<a id=\"credit_scores\"></a>\nCredit scores are important metrics for assesing the overall level of risk. In this section we will analyze the level of risk as a whole and how many loans were bad loans by the type of grade received in the credit score of the customer.\n\n<h4> What we need to know: </h4>\n<ul> \n<li> The lower the grade of the credit score, the higher the risk for investors. </li>\n<li> There are different factors that influence on the level of risk of the loan.</li>\n</ul>\n\n<h4> Summary: </h4>\n<ul>\n<li> The scores that has a lower grade received a larger amounts of loans (which might had contributed to a higher level of risk). </li>\n<li> Logically, the <b>lower the grade the higher the interest</b> the customer had to pay back to investors.</li>\n<li> Interstingly, customers with a <b>grade</b> of \"C\" were more likely to default on the loan </li>\n<ul>"},{"metadata":{"_cell_guid":"72c471f0-91a8-4e19-bd7c-16704f30501c","_uuid":"0a0df1473572f180ac8a4c94b9fca66ed80634f6","trusted":true},"cell_type":"code","source":"# Let's visualize how many loans were issued by creditscore\nf, ((ax1, ax2)) = plt.subplots(1, 2)\ncmap = plt.cm.coolwarm\n\nby_credit_score = df.groupby(['year', 'grade']).loan_amount.mean()\nby_credit_score.unstack().plot(legend=False, ax=ax1, figsize=(14, 4), colormap=cmap)\nax1.set_title('Loans issued by Credit Score', fontsize=14)\n    \n    \nby_inc = df.groupby(['year', 'grade']).interest_rate.mean()\nby_inc.unstack().plot(ax=ax2, figsize=(14, 4), colormap=cmap)\nax2.set_title('Interest Rates by Credit Score', fontsize=14)\n\nax2.legend(bbox_to_anchor=(-1.0, -0.3, 1.7, 0.1), loc=5, prop={'size':12},\n           ncol=7, mode=\"expand\", borderaxespad=0.)","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"ba20dded-74a0-48de-bc66-1530abdd7cb2","_uuid":"d9d859fd942b7ed538a680a2fbe0cc05c997cc50","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,12))\n\nax1 = fig.add_subplot(221)\nax2 = fig.add_subplot(222)\nax3 = fig.add_subplot(212)\n\ncmap = plt.cm.coolwarm_r\n\nloans_by_region = df.groupby(['grade', 'loan_condition']).size()\nloans_by_region.unstack().plot(kind='bar', stacked=True, colormap=cmap, ax=ax1, grid=False)\nax1.set_title('Type of Loans by Grade', fontsize=14)\n\n\nloans_by_grade = df.groupby(['sub_grade', 'loan_condition']).size()\nloans_by_grade.unstack().plot(kind='bar', stacked=True, colormap=cmap, ax=ax2, grid=False)\nax2.set_title('Type of Loans by Sub-Grade', fontsize=14)\n\nby_interest = df.groupby(['year', 'loan_condition']).interest_rate.mean()\nby_interest.unstack().plot(ax=ax3, colormap=cmap)\nax3.set_title('Average Interest rate by Loan Condition', fontsize=14)\nax3.set_ylabel('Interest Rate (%)', fontsize=12)","execution_count":26,"outputs":[]},{"metadata":{"_cell_guid":"265914e0-b326-4b6c-8cad-a55ebce491b7","_uuid":"5a63504f01a6e3e7088f57138754724e166b4d7e"},"cell_type":"markdown","source":"<h2>What Determines a Bad Loan </h2>\n<a id=\"determines_bad_loan\"></a>\nMy main aim in this section is to find the main factors that causes for a loan to be considered a <b>\"Bad Loan\"</b>. Logically, we could assume that factors such as a low credit grade or a high debt to income could be possible contributors in determining whether a loan is at a high risk of being defaulted. <br><br>\n\n<h4> What we need to know: </h4>\n<ul>\n<li> There might be possible factors that contribute in whether a loan is bad or not. </li>\n<li> Factors that increase risk include: low annual income, high debt to income, high interest rates, low grade, among others. </li>\n</ul>\n\n<h4> Summary: </h4>\n<ul>\n<li> The types of bad loans in the last year are having a tendency to<b> decline</b>, except for late payments (might indicate an economical recovery.) </li>\n<li> <b>Mortgage </b> was the variable from the home ownership column that used the highest amount borrowed within loans that were considered to be bad.</li>\n<li> There is a slight <b>increase</b> on people who have mortgages that are applying for a loan.</li>\n<li>People who have a mortgage (depending on other factors as well within the mortgage) are more likely to ask for <bhigher loan amounts than other people who have other types of home ownerships. </li>\n</ul>"},{"metadata":{"_cell_guid":"19a36a8e-0863-45a2-9d31-5e71a038aff5","_uuid":"ce76bb007c77bf64c91ef010f11c8f5a49aa8817","collapsed":true,"trusted":true},"cell_type":"code","source":"# Just get me the numeric variables\nnumeric_variables = df.select_dtypes(exclude=[\"object\"])","execution_count":27,"outputs":[]},{"metadata":{"_cell_guid":"8084478f-0d2d-4922-87a3-f6fd002465da","_uuid":"562f902591f808328462b310066571f8e2f0a0e5","trusted":true},"cell_type":"code","source":"# We will use df_correlations dataframe to analyze our correlations.\n\n\ndf_correlations = df.corr()\n\n\ntrace = go.Heatmap(z=df_correlations.values,\n                   x=df_correlations.columns,\n                   y=df_correlations.columns,\n                  colorscale=[[0.0, 'rgb(165,0,38)'], \n                              [0.1111111111111111, 'rgb(215,48,39)'], \n                              [0.2222222222222222, 'rgb(244,109,67)'], \n                              [0.3333333333333333, 'rgb(253,174,97)'], \n                              [0.4444444444444444, 'rgb(254,224,144)'], \n                              [0.5555555555555556, 'rgb(224,243,248)'], \n                              [0.6666666666666666, 'rgb(171,217,233)'], \n                              [0.7777777777777778, 'rgb(116,173,209)'], \n                              [0.8888888888888888, 'rgb(69,117,180)'], \n                              [1.0, 'rgb(49,54,149)']],\n            colorbar = dict(\n            title = 'Level of Correlation',\n            titleside = 'top',\n            tickmode = 'array',\n            tickvals = [-0.52,0.2,0.95],\n            ticktext = ['Negative Correlation','Low Correlation','Positive Correlation'],\n            ticks = 'outside'\n        )\n                  )\n\n\nlayout = {\"title\": \"Correlation Heatmap\"}\ndata=[trace]\n\nfig = dict(data=data, layout=layout)\niplot(fig, filename='labelled-heatmap')","execution_count":28,"outputs":[]},{"metadata":{"_cell_guid":"3d778b66-27da-4933-8b30-1106056e8111","_uuid":"bad06adcb774d5515d7fe653a247d9aa7f5a71e1"},"cell_type":"markdown","source":"This data looks a little but messy maybe if we focus our correlation heatmap into columns that are more worth it we might actually see a trend with the **condition of the loan**."},{"metadata":{"_cell_guid":"1ac9ade7-50ca-413b-a3a4-f0024c2a7005","_uuid":"93234728cf730963285429c2ae2a4a610e0c5588","trusted":true},"cell_type":"code","source":"title = 'Bad Loans: Loan Statuses'\n\nlabels = bad_loan # All the elements that comprise a bad loan.\n\nlen(labels)\ncolors = ['rgba(236, 112, 99, 1)', 'rgba(235, 152, 78, 1)', 'rgba(52, 73, 94, 1)', 'rgba(128, 139, 150, 1)',\n         'rgba(255, 87, 51, 1)', 'rgba(255, 195, 0, 1)']\n\nmode_size = [8,8,8,8,8,8]\n\nline_size = [2,2,2,2,2,2]\n\nx_data = [\n    sorted(df['year'].unique().tolist()),\n    sorted(df['year'].unique().tolist()),\n    sorted(df['year'].unique().tolist()),\n    sorted(df['year'].unique().tolist()), \n    sorted(df['year'].unique().tolist()),\n    sorted(df['year'].unique().tolist()),\n]\n\n# type of loans\ncharged_off = df['loan_amount'].loc[df['loan_status'] == 'Charged Off'].values.tolist()\ndefaults = df['loan_amount'].loc[df['loan_status'] == 'Default'].values.tolist()\nnot_credit_policy = df['loan_amount'].loc[df['loan_status'] == 'Does not meet the credit policy. Status:Charged Off'].values.tolist()\ngrace_period = df['loan_amount'].loc[df['loan_status'] == 'In Grace Period'].values.tolist()\nshort_late = df['loan_amount'].loc[df['loan_status'] == 'Late (16-30 days)'].values.tolist()\nlong_late = df['loan_amount'].loc[df['loan_status'] == 'Late (31-120 days)'].values.tolist()\n\ny_data = [\n    charged_off,\n    defaults,\n    not_credit_policy,\n    grace_period,\n    short_late,\n    long_late,\n]\n\np_charged_off = go.Scatter(\n    x = x_data[0],\n    y = y_data[0],\n    name = 'A. Charged Off',\n    line = dict(\n        color = colors[0],\n        width = 3,\n        dash='dash')\n)\n\np_defaults = go.Scatter(\n    x = x_data[1],\n    y = y_data[1],\n    name = 'A. Defaults',\n    line = dict(\n        color = colors[1],\n        width = 3,\n        dash='dash')\n)\n\np_credit_policy = go.Scatter(\n    x = x_data[2],\n    y = y_data[2],\n    name = 'Not Meet C.P',\n    line = dict(\n        color = colors[2],\n        width = 3,\n        dash='dash')\n)\n\np_graced = go.Scatter(\n    x = x_data[3],\n    y = y_data[3],\n    name = 'A. Graced Period',\n    line = dict(\n        color = colors[3],\n        width = 3,\n        dash='dash')\n)\n\np_short_late = go.Scatter(\n    x = x_data[4],\n    y = y_data[4],\n    name = 'Late (16-30 days)',\n    line = dict(\n        color = colors[4],\n        width = 3,\n        dash='dash')\n)\n\np_long_late = go.Scatter(\n    x = x_data[5],\n    y = y_data[5],\n    name = 'Late (31-120 days)',\n    line = dict(\n        color = colors[5],\n        width = 3,\n        dash='dash')\n)\n\n\n\n\ndata=[p_charged_off, p_defaults, p_credit_policy, p_graced, p_short_late, p_long_late]\n\nlayout = dict(title = 'Types of Bad Loans <br> (Amount Borrowed Throughout the Years)',\n              xaxis = dict(title = 'Year'),\n              yaxis = dict(title = 'Amount Issued'),\n              )\n\nfig = dict(data=data, layout=layout)\n\niplot(fig, filename='line-mode')","execution_count":29,"outputs":[]},{"metadata":{"_cell_guid":"fa9e904b-b8be-4e54-b260-415497101239","_uuid":"9f4988abfc9615bf59f35044c3b03aa75ab21126","trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nplt.figure(figsize=(18,18))\n\n# Create a dataframe for bad loans\nbad_df = df.loc[df['loan_condition'] == 'Bad Loan']\n\nplt.subplot(211)\ng = sns.boxplot(x='home_ownership', y='loan_amount', hue='loan_condition',\n               data=bad_df, color='r')\n\ng.set_xticklabels(g.get_xticklabels(),rotation=45)\ng.set_xlabel(\"Type of Home Ownership\", fontsize=12)\ng.set_ylabel(\"Loan Amount\", fontsize=12)\ng.set_title(\"Distribution of Amount Borrowed \\n by Home Ownership\", fontsize=16)\n\n\n\nplt.subplot(212)\ng1 = sns.boxplot(x='year', y='loan_amount', hue='home_ownership',\n               data=bad_df, palette=\"Set3\")\ng1.set_xticklabels(g1.get_xticklabels(),rotation=45)\ng1.set_xlabel(\"Type of Home Ownership\", fontsize=12)\ng1.set_ylabel(\"Loan Amount\", fontsize=12)\ng1.set_title(\"Distribution of Amount Borrowed \\n through the years\", fontsize=16)\n\n\nplt.subplots_adjust(hspace = 0.6, top = 0.8)\n\nplt.show()","execution_count":30,"outputs":[]},{"metadata":{"_cell_guid":"9cbebcdb-0445-44dd-960c-c4dbc734929c","_uuid":"fa94049d344e0cd0407274ff094fd28302b4cd73"},"cell_type":"markdown","source":"## Defaulted Loans and Level of Risk:\n<a id=\"defaulted_loans\"></a>\nFrom all the bad loans the one we are most interested about are the loans that are defaulted. Therefore, in this section we will implement an in-depth analysis of these types of Loans and see if we can gain any insight as to which features have a high correlation with the loan being defaulted.\n\n## Main Aim:\n<ul>\n<li> Determine patters that will allow us to understand somehow factors that contribute to a loan being <b>defaulted</b> </li>\n</ul>\n\n## Summary:\n<ul>\n<li>In the last year recorded, the <b>Midwest </b>  and <b> SouthEast </b> regions had the most defaults. </li>\n<li>Loans that have a <b>high interest rate</b>(above 13.23%) are more likely to become a <b>bad loan </b>. </li>\n<li>Loans that have a longer <b> maturity date (60 months) </b> are more likely to be a bad loan. </li>\n</ul>\n\n"},{"metadata":{"_cell_guid":"21cefe96-b94e-4fc9-966d-7b7e84b23f1e","_uuid":"3ebf01c55a68109fca1194f13c7eabb6e6d2bd23","trusted":true},"cell_type":"code","source":"\n# Get the loan amount for loans that were defaulted by each region.\nnorthe_defaults = df['loan_amount'].loc[(df['region'] == 'NorthEast') & (df['loan_status'] == 'Default')].values.tolist()\nsouthw_defaults = df['loan_amount'].loc[(df['region'] == 'SouthWest') & (df['loan_status'] == 'Default')].values.tolist()\nsouthe_defaults = df['loan_amount'].loc[(df['region'] == 'SouthEast') & (df['loan_status'] == 'Default')].values.tolist()\nwest_defaults = df['loan_amount'].loc[(df['region'] == 'West') & (df['loan_status'] == 'Default')].values.tolist()\nmidw_defaults = df['loan_amount'].loc[(df['region'] == 'MidWest') & (df['loan_status'] == 'Default')].values.tolist()\n\n# Cumulative Values\ny0_stck=northe_defaults\ny1_stck=[y0+y1 for y0, y1 in zip(northe_defaults, southw_defaults)]\ny2_stck=[y0+y1+y2 for y0, y1, y2 in zip(northe_defaults, southw_defaults, southe_defaults)]\ny3_stck=[y0+y1+y2+y3 for y0, y1, y2, y3 in zip(northe_defaults, southw_defaults, southe_defaults, west_defaults)]\ny4_stck=[y0+y1+y2+y3+y4 for y0, y1, y2, y3, y4 in zip(northe_defaults, southw_defaults, southe_defaults, west_defaults, midw_defaults)] \n\n# Make original values strings and add % for hover text\ny0_txt=['$' + str(y0) for y0 in northe_defaults]\ny1_txt=['$' + str(y1) for y1 in southw_defaults]\ny2_txt=['$' + str(y2) for y2 in southe_defaults]\ny3_txt=['$' + str(y3) for y3 in west_defaults]\ny4_txt=['$'+ str(y4) for y4 in midw_defaults]\n\nyear = sorted(df[\"year\"].unique().tolist())\n\nNorthEast_defaults = go.Scatter(\n    x= year,\n    y= y0_stck,\n    text=y0_txt,\n    hoverinfo='x+text',\n    name='NorthEast',\n    mode= 'lines',\n    line=dict(width=0.5,\n             color='rgb(131, 90, 241)'),\n    fill='tonexty'\n)\nSouthWest_defaults = go.Scatter(\n    x=year,\n    y=y1_stck,\n    text=y1_txt,\n    hoverinfo='x+text',\n    name='SouthWest',\n    mode= 'lines',\n    line=dict(width=0.5,\n             color='rgb(255, 140, 0)'),\n    fill='tonexty'\n)\n\nSouthEast_defaults = go.Scatter(\n    x= year,\n    y= y2_stck,\n    text=y2_txt,\n    hoverinfo='x+text',\n    name='SouthEast',\n    mode= 'lines',\n    line=dict(width=0.5,\n             color='rgb(240, 128, 128)'),\n    fill='tonexty'\n)\n\nWest_defaults = go.Scatter(\n    x= year,\n    y= y3_stck,\n    text=y3_txt,\n    hoverinfo='x+text',\n    name='West',\n    mode= 'lines',\n    line=dict(width=0.5,\n             color='rgb(135, 206, 235)'),\n    fill='tonexty'\n)\n\nMidWest_defaults = go.Scatter(\n    x= year,\n    y= y4_stck,\n    text=y4_txt,\n    hoverinfo='x+text',\n    name='MidWest',\n    mode= 'lines',\n    line=dict(width=0.5,\n             color='rgb(240, 230, 140)'),\n    fill='tonexty'\n    )\n\n\ndata = [NorthEast_defaults, SouthWest_defaults, SouthEast_defaults, West_defaults, MidWest_defaults]\n\nlayout = dict(title = 'Amount Defaulted by Region',\n              xaxis = dict(title = 'Year'),\n              yaxis = dict(title = 'Amount Defaulted')\n             )\n\nfig = dict(data=data, layout=layout)\n              \niplot(fig, filename='basic-area-no-bound')","execution_count":31,"outputs":[]},{"metadata":{"_cell_guid":"a054995e-1b03-4ebf-bf0d-f1e73c3bc4d0","_uuid":"0b92e46f6497e3879c38341e03f12725bf5a9957"},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"04332c5a-48ad-4b93-8925-a658dce50e40","_uuid":"1b66129c72b78775afad04857630c908089a7ca9","trusted":true},"cell_type":"code","source":"df['interest_rate'].describe()\n# Average interest is 13.26% Anything above this will be considered of high risk let's see if this is true.\ndf['interest_payments'] = np.nan\nlst = [df]\n\nfor col in lst:\n    col.loc[col['interest_rate'] <= 13.23, 'interest_payments'] = 'Low'\n    col.loc[col['interest_rate'] > 13.23, 'interest_payments'] = 'High'\n    \ndf.head()","execution_count":32,"outputs":[]},{"metadata":{"_cell_guid":"6afd4368-f754-40f0-9e4d-cbda61ebbabe","_uuid":"248f8252aaa9e0024e6a57f59a58d44430808a00","trusted":true},"cell_type":"code","source":"df['term'].value_counts()","execution_count":33,"outputs":[]},{"metadata":{"_cell_guid":"9ef845e2-3cc9-489f-a5d1-4e43d0726a4b","_uuid":"dea4755d7bcfadfdca96f1adbeb94d44d3386b00","trusted":true},"cell_type":"code","source":"from scipy.stats import norm\n\nplt.figure(figsize=(20,10))\n\npalette = ['#009393', '#930000']\nplt.subplot(221)\nax = sns.countplot(x='interest_payments', data=df, \n                  palette=palette, hue='loan_condition')\n\nax.set_title('The impact of interest rate \\n on the condition of the loan', fontsize=14)\nax.set_xlabel('Level of Interest Payments', fontsize=12)\nax.set_ylabel('Count')\n\nplt.subplot(222)\nax1 = sns.countplot(x='interest_payments', data=df, \n                   palette=palette, hue='term')\n\nax1.set_title('The impact of maturity date \\n on interest rates', fontsize=14)\nax1.set_xlabel('Level of Interest Payments', fontsize=12)\nax1.set_ylabel('Count')\n\n\nplt.subplot(212)\nlow = df['loan_amount'].loc[df['interest_payments'] == 'Low'].values\nhigh = df['loan_amount'].loc[df['interest_payments'] == 'High'].values\n\n\nax2= sns.distplot(low, color='#009393', label='Low Interest Payments', fit=norm, fit_kws={\"color\":\"#483d8b\"}) # Dark Blue Norm Color\nax3 = sns.distplot(high, color='#930000', label='High Interest Payments', fit=norm, fit_kws={\"color\":\"#c71585\"}) #  Red Norm Color\nplt.axis([0, 36000, 0, 0.00016])\nplt.legend()\n\n\nplt.show()","execution_count":34,"outputs":[]},{"metadata":{"_cell_guid":"aa0fd7e2-870d-401d-96e5-a7c504157872","_uuid":"70536045c4c088b23e82cde2d7643d4289adb0f9"},"cell_type":"markdown","source":"## Interest Rate by Loan Status:\nThe main aim in this section is to compare the average interest rate for the loan status belonging to each type of loans (Good loan or bad loan) and see if there is any significant difference in the average of interest rate for each of the groups.\n\n## Summary: \n<ul>\n<li> <b> Bad Loans: </b>  Most of the loan statuses belonging to this group pay a interest ranging from 15% - 16%. </li>\n<li><b>Good Loans:</b> Most of the loan statuses belonging to this group pay interest ranging from 12% - 13%.  </li>\n<li>There has to be a better assesment of risk since there is not that much of a difference in interest payments from <b>Good Loans</b> and <b>Bad Loans</b>. </li>\n<li> Remember, most loan statuses are <b>Current</b> so there is a risk that at the end of maturity some of these loans might become bad loans. </li>\n</ul>\n\n<br>\n\n*Credits to Zhiwen for providing an important aspect of the analysis (Relationship of interest rates and loan condition).*"},{"metadata":{"_cell_guid":"ba05f3a3-af80-496c-817a-35a34cbfe7bc","_uuid":"c02adf19c2c71e052cbe3aeab7d8bd6663ef289d","trusted":true},"cell_type":"code","source":"import plotly.plotly as py\nimport plotly.graph_objs as go\n\n# Interest rate good loans\navg_fully_paid = round(np.mean(df['interest_rate'].loc[df['loan_status'] == 'Fully Paid'].values), 2)\navg_current = round(np.mean(df['interest_rate'].loc[df['loan_status'] == 'Current'].values), 2) \navg_issued = round(np.mean(df['interest_rate'].loc[df['loan_status'] == 'Issued'].values), 2)\navg_long_fully_paid = round(np.mean(df['interest_rate'].loc[df['loan_status'] == 'Does not meet the credit policy. Status:Fully Paid'].values), 2)\n\n\n\n# Interest rate bad loans\n\navg_default_rates = round(np.mean(df['interest_rate'].loc[df['loan_status'] == 'Default'].values), 2)\navg_charged_off = round(np.mean(df['interest_rate'].loc[df['loan_status'] == 'Charged Off'].values), 2)\navg_long_charged_off = round(np.mean(df['interest_rate'].loc[df['loan_status'] == 'Does not meet the credit policy. Status:Charged Off'].values), 2)\navg_grace_period = round(np.mean(df['interest_rate'].loc[df['loan_status'] == 'In Grace Period'].values), 2)\navg_short_late = round(np.mean(df['interest_rate'].loc[df['loan_status'] == 'Late (16-30 days)'].values), 2)\navg_long_late = round(np.mean(df['interest_rate'].loc[df['loan_status'] == 'Late (31-120 days)'].values), 2)\n\n\n# Take to a dataframe\n\ndata = [\n    go.Scatterpolar(\n        mode='line+markers',\n      r = [avg_fully_paid, avg_current, avg_issued, avg_long_fully_paid],\n      theta = ['Fully Paid', 'Current', 'Issued', 'No C.P. Fully Paid'],\n      fill = 'toself',\n      name = 'Good Loans',\n        line = dict(\n        color = \"#63AF63\"\n      ),\n      marker = dict(\n        color = \"#B3FFB3\",\n        symbol = \"square\",\n        size = 8\n      ),\n      subplot = \"polar\",\n    ),\n    go.Scatterpolar(\n        mode='line+markers',\n      r = [avg_default_rates, avg_charged_off, avg_long_charged_off, avg_grace_period, avg_short_late, avg_long_late],\n      theta = ['Default Rate', 'Charged Off', 'C.P. Charged Off', 'In Grace Period', 'Late (16-30 days)', 'Late (31-120 days)'],\n      fill = 'toself',\n      name = 'Bad Loans',\n        line = dict(\n        color = \"#C31414\"\n      ),\n      marker = dict(\n        color = \"#FF5050\",\n        symbol = \"square\",\n        size = 8\n      ),\n      subplot = \"polar2\"\n    )\n]\n\nlayout = go.Layout(\n    title=\"Average Interest Rates <br> Loan Status Distribution\",\n    showlegend = False,\n     paper_bgcolor = \"\trgb(255, 248, 243)\",\n    polar = dict(\n      domain = dict(\n        x = [0,0.4],\n        y = [0,1]\n      ),\n      radialaxis = dict(\n        tickfont = dict(\n          size = 8\n        )\n      ),\n      angularaxis = dict(\n        tickfont = dict(\n          size = 8\n        ),\n        rotation = 90,\n        direction = \"counterclockwise\"\n      )\n    ),\n    polar2 = dict(\n      domain = dict(\n        x = [0.6,1],\n        y = [0,1]\n      ),\n      radialaxis = dict(\n        tickfont = dict(\n          size = 8\n        )\n      ),\n      angularaxis = dict(\n        tickfont = dict(\n          size = 8\n        ),\n        rotation = 90,\n        direction = \"clockwise\"\n      ),\n    )\n)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='polar/directions')","execution_count":73,"outputs":[]},{"metadata":{"_cell_guid":"86754a74-044f-4a11-99dd-558c698348c8","_uuid":"b7a23547860a6523e22df9d2ece313c10dc64190"},"cell_type":"markdown","source":"## Feature Engineering and Neural Network:\n**Steps:**\n<ul>\n<li> There are <b> features </b> that are redundant (as show in the beginning of this kernel in the distribution subplots) having no effect towards the \"loan_condition\" label so we need to <b> drop these features</b>.</li><br>\n<li> Use <b>StrattifiedShuffleSplit</b> to have approximately the same ratio of bad loans compared to good loans in both training and testing data. Remember that over 92% of the loans are considered good loans so it is important to have this same ration across training and testing sets. </li>\n<li> <b>Scale </b> numeric features and <b>encode</b> categorical features from our dataframe. </li>\n<li> Run our Neural Network containing the number of inputs, 2 hidden layers (first: 15 nodes, second: 5 nodes) and the number of outputs which is equivalent to 2.</li>\n</ul>"},{"metadata":{"_cell_guid":"e46bb65f-4737-4b43-bbca-59adca499918","_uuid":"5ca383c5dc47f8cd8d934013f934eda254fd1ddc","collapsed":true,"trusted":true},"cell_type":"code","source":"# Let's make a copy of the dataframe to avoid confusion.\ncomplete_df = df.copy()","execution_count":36,"outputs":[]},{"metadata":{"_cell_guid":"a1e68755-64ff-40bc-b8f7-d299ad21f58a","_uuid":"2472af317cf864bcdb0faae24af46132607d6bfb","collapsed":true,"trusted":true},"cell_type":"code","source":"# Handling Missing Numeric Values\n\n# Transform Missing Values for numeric dataframe\n# Nevertheless check what these variables mean tomorrow in the morning.\nfor col in ('dti_joint', 'annual_inc_joint', 'il_util', 'mths_since_rcnt_il', 'open_acc_6m', 'open_il_6m', 'open_il_12m',\n           'open_il_24m', 'inq_last_12m', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util', 'inq_fi', 'total_cu_tl',\n           'mths_since_last_record', 'mths_since_last_major_derog', 'mths_since_last_delinq', 'total_bal_il', 'tot_coll_amt',\n           'tot_cur_bal', 'total_rev_hi_lim', 'revol_util', 'collections_12_mths_ex_med', 'open_acc', 'inq_last_6mths',\n           'verification_status_joint', 'acc_now_delinq'):\n    complete_df[col] = complete_df[col].fillna(0)\n    \n\n\n# # Get the mode of next payment date and last payment date and the last date credit amount was pulled   \ncomplete_df[\"next_pymnt_d\"] = complete_df.groupby(\"region\")[\"next_pymnt_d\"].transform(lambda x: x.fillna(x.mode))\ncomplete_df[\"last_pymnt_d\"] = complete_df.groupby(\"region\")[\"last_pymnt_d\"].transform(lambda x: x.fillna(x.mode))\ncomplete_df[\"last_credit_pull_d\"] = complete_df.groupby(\"region\")[\"last_credit_pull_d\"].transform(lambda x: x.fillna(x.mode))\ncomplete_df[\"earliest_cr_line\"] = complete_df.groupby(\"region\")[\"earliest_cr_line\"].transform(lambda x: x.fillna(x.mode))\n\n# # Get the mode on the number of accounts in which the client is delinquent\ncomplete_df[\"pub_rec\"] = complete_df.groupby(\"region\")[\"pub_rec\"].transform(lambda x: x.fillna(x.median()))\n\n# # Get the mean of the annual income depending in the region the client is located.\ncomplete_df[\"annual_income\"] = complete_df.groupby(\"region\")[\"annual_income\"].transform(lambda x: x.fillna(x.mean()))\n\n# Get the mode of the  total number of credit lines the borrower has \ncomplete_df[\"total_acc\"] = complete_df.groupby(\"region\")[\"total_acc\"].transform(lambda x: x.fillna(x.median()))\n\n# Mode of credit delinquencies in the past two years.\ncomplete_df[\"delinq_2yrs\"] = complete_df.groupby(\"region\")[\"delinq_2yrs\"].transform(lambda x: x.fillna(x.mean()))","execution_count":37,"outputs":[]},{"metadata":{"_cell_guid":"b59e358a-8beb-4a52-b104-86a1129e6016","_uuid":"cfb46ba6476590b51240d45395fdf7448035aa42","collapsed":true,"trusted":true},"cell_type":"code","source":"# Drop these variables before scaling but don't drop these when we perform feature engineering on missing values.\n# Columns to delete or fix: earliest_cr_line, last_pymnt_d, next_pymnt_d, last_credit_pull_d, verification_status_joint\n\n# ---->>>> Fix the problems shown during scaling with the columns above.\n\ncomplete_df.drop(['issue_d', 'income_category', 'region', 'year', 'emp_length', 'loan_condition_int',\n                 'earliest_cr_line', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d', \n                 'verification_status_joint', 'emp_length_int', 'total_rec_prncp', 'funded_amount', 'investor_funds', \n                 'sub_grade', 'complete_date', 'loan_status', 'interest_payments', \n                 'initial_list_status', 'out_prncp', 'out_prncp_inv', 'total_pymnt',\n               'total_pymnt_inv', 'total_rec_int', 'total_rec_late_fee', 'recoveries',\n               'collection_recovery_fee', 'last_pymnt_amnt',\n               'collections_12_mths_ex_med', 'mths_since_last_major_derog',\n               'policy_code', 'application_type', 'annual_inc_joint', 'dti_joint',\n               'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'open_acc_6m',\n               'open_il_6m', 'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il',\n               'total_bal_il', 'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc',\n               'all_util', 'total_rev_hi_lim', 'inq_fi', 'total_cu_tl', 'inq_last_12m'], axis=1, inplace=True)","execution_count":38,"outputs":[]},{"metadata":{"_cell_guid":"e193e220-0dd5-4dae-b081-e825d7e8cff1","_uuid":"6147c109241c4a9d9a0c67770f4c5294b0408f77","trusted":true},"cell_type":"code","source":"complete_df.columns","execution_count":39,"outputs":[]},{"metadata":{"_cell_guid":"d03e37e6-35e7-48b6-a7a8-57b515f391a8","_uuid":"134b552c4c33951fd33fd4f812737d83cfae4df9","trusted":true},"cell_type":"code","source":"complete_df.isnull().sum().max() # Maximum number of nulls.","execution_count":40,"outputs":[]},{"metadata":{"_cell_guid":"51040128-3010-4815-afdc-f68deb47753e","_uuid":"357659a528817baef198caec20cd52182b696ffa","trusted":true},"cell_type":"code","source":"# We should have a raio of 92% of good loans and 7% of bad loans\n# We can do this with stratified sampling\ncomplete_df['loan_condition'].value_counts()/len(df)","execution_count":41,"outputs":[]},{"metadata":{"_cell_guid":"0656a46f-77e9-4a8e-af8f-8b75010d47ba","_uuid":"96c13f3db727269c9284b528d1c8c861233f2c74"},"cell_type":"markdown","source":"The purpose of the code below is to have the same ratio across our training and test sets."},{"metadata":{"_cell_guid":"19d69a00-f76e-4ec2-aced-5da11baedffb","_uuid":"b980cb9a5f4aed482cbb886be2c7ca5d56da7608","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nstratified = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_set, test_set in stratified.split(complete_df, complete_df[\"loan_condition\"]):\n    stratified_train = complete_df.loc[train_set]\n    stratified_test = complete_df.loc[test_set]\n    \nprint('Train set ratio \\n', stratified_train[\"loan_condition\"].value_counts()/len(df))\nprint('Test set ratio \\n', stratified_test[\"loan_condition\"].value_counts()/len(df))","execution_count":42,"outputs":[]},{"metadata":{"_cell_guid":"f1e9321f-4323-4f82-a592-e7a896c30e6a","_uuid":"24aa4fc2d8b3458f03519bfdad9e8f84f8286625","collapsed":true,"trusted":true},"cell_type":"code","source":"train_df = stratified_train\ntest_df = stratified_test\n\n# Let's Shuffle the data\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\ntest_df = test_df.sample(frac=1).reset_index(drop=True)\n\n\n# Train Dataset\nX_train = train_df.drop('loan_condition', axis=1)\ny_train = train_df['loan_condition']\n\n# Test Dataset\nX_test = test_df.drop('loan_condition', axis=1)\ny_test = test_df['loan_condition']","execution_count":43,"outputs":[]},{"metadata":{"_cell_guid":"6644f676-7551-40b3-a838-a5897ffcc674","_uuid":"d3c3d5af87db89b0a8b0bfffcf0c6eb3522daa93","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import sparse\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"Encode categorical features as a numeric array.\n    The input to this transformer should be a matrix of integers or strings,\n    denoting the values taken on by categorical (discrete) features.\n    The features can be encoded using a one-hot aka one-of-K scheme\n    (``encoding='onehot'``, the default) or converted to ordinal integers\n    (``encoding='ordinal'``).\n    This encoding is needed for feeding categorical data to many scikit-learn\n    estimators, notably linear models and SVMs with the standard kernels.\n    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n    Parameters\n    ----------\n    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n        The type of encoding to use (default is 'onehot'):\n        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n          (or also called 'dummy' encoding). This creates a binary column for\n          each category and returns a sparse matrix.\n        - 'onehot-dense': the same as 'onehot' but returns a dense array\n          instead of a sparse matrix.\n        - 'ordinal': encode the features as ordinal integers. This results in\n          a single column of integers (0 to n_categories - 1) per feature.\n    categories : 'auto' or a list of lists/arrays of values.\n        Categories (unique values) per feature:\n        - 'auto' : Determine categories automatically from the training data.\n        - list : ``categories[i]`` holds the categories expected in the ith\n          column. The passed categories are sorted before encoding the data\n          (used categories can be found in the ``categories_`` attribute).\n    dtype : number type, default np.float64\n        Desired dtype of output.\n    handle_unknown : 'error' (default) or 'ignore'\n        Whether to raise an error or ignore if a unknown categorical feature is\n        present during transform (default is to raise). When this is parameter\n        is set to 'ignore' and an unknown category is encountered during\n        transform, the resulting one-hot encoded columns for this feature\n        will be all zeros.\n        Ignoring unknown categories is not supported for\n        ``encoding='ordinal'``.\n    Attributes\n    ----------\n    categories_ : list of arrays\n        The categories of each feature determined during fitting. When\n        categories were specified manually, this holds the sorted categories\n        (in order corresponding with output of `transform`).\n    Examples\n    --------\n    Given a dataset with three features and two samples, we let the encoder\n    find the maximum value per feature and transform the data to a binary\n    one-hot encoding.\n    >>> from sklearn.preprocessing import CategoricalEncoder\n    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n    ... # doctest: +ELLIPSIS\n    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n              encoding='onehot', handle_unknown='ignore')\n    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n    See also\n    --------\n    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n      integer ordinal features. The ``OneHotEncoder assumes`` that input\n      features take on values in the range ``[0, max(feature)]`` instead of\n      using the unique values.\n    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n      dictionary items (also handles string-valued features).\n    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n      encoding of dictionary items or strings.\n    \"\"\"\n\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","execution_count":44,"outputs":[]},{"metadata":{"_cell_guid":"6d3c2522-424e-49ce-87fd-8426513471b8","_uuid":"726ff1f33ce1d6b62b88dd1a6cf3326211da201c","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# A class to select numerical or categorical columns \n# since Scikit-Learn doesn't handle DataFrames yet\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","execution_count":45,"outputs":[]},{"metadata":{"_cell_guid":"ec0dd733-78a9-4ba8-9f59-90a1b3438e29","_uuid":"48c4010579e607b3ad0ef04e46cb25ddd8929fd8","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler\n\n# Columns to delete or fix: earliest_cr_line, last_pymnt_d, next_pymnt_d, last_credit_pull_d, verification_status_joint\n\nnumeric = X_train.select_dtypes(exclude=[\"object\"])\ncategorical = X_train.select_dtypes([\"object\"])\n\nnumeric_pipeline = Pipeline([\n    ('selector', DataFrameSelector(numeric.columns.tolist())),\n    ('scaler', StandardScaler()),\n])\n\ncategorical_pipeline = Pipeline([\n    ('selector', DataFrameSelector(categorical.columns.tolist())), # We will have to write the categorical columns manually and see if it works.\n    ('encoder', CategoricalEncoder(encoding=\"onehot-dense\")),\n])\n\n# Combine both Pipelines into one array\ncombined_pipeline = FeatureUnion(transformer_list=[\n    ('numeric_pipeline', numeric_pipeline),\n    ('categorical_pipeline', categorical_pipeline)\n])\n\nX_train = combined_pipeline.fit_transform(X_train)\nX_test = combined_pipeline.fit_transform(X_test)","execution_count":46,"outputs":[]},{"metadata":{"_cell_guid":"ab527ddf-e962-49fa-b79e-09e07b4e6dd2","_uuid":"4a9f4d90bc3c2327e675dd647ff787b8b254aa71","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nencode = LabelEncoder()\ny_train = encode.fit_transform(y_train)\ny_test = encode.fit_transform(y_test)","execution_count":47,"outputs":[]},{"metadata":{"_cell_guid":"2660e289-e490-4b20-9c28-bd933fb49659","_uuid":"e97667a44bd6b6a1e6c1e0f3d18c00e735d601d9","trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\n\n# Reset the graph for Tensorboard\ndef reset_graph(seed=42):\n    tf.reset_default_graph()\n    tf.set_random_seed(seed)\n    np.random.seed(seed)\n    \n\n\n# Variables\nn_inputs = X_train.shape[1]\nn_hidden1 = 15\nn_hidden2 = 5\nn_outputs = 2\n\n# Reset the tensorboard graph\nreset_graph()\n\n\n# Placeholders\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\ny = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n\n\n# Structure of the Neural Network\nwith tf.name_scope(\"dnn\"):\n    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n                             activation=tf.nn.relu)\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n                             activation=tf.nn.relu)\n    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n\n# Cost Function\nwith tf.name_scope(\"loss\"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n                                                     logits=logits) # Difference from logits and actual y values\n    loss = tf.reduce_mean(xentropy) # Get the average of the loss for each instance.\n\n# Gradient Descent\nlearning_rate = 0.01\n\nwith tf.name_scope(\"train\"):\n    optimization = tf.train.GradientDescentOptimizer(learning_rate) # Determine the level of steps in gradient descent process.\n    training_op = optimization.minimize(loss) # Get the training set with parameters that obtain the minimum loss.\n\n# Evaluation\nwith tf.name_scope(\"eval\"):\n    correct = tf.nn.in_top_k(logits, y, 1) # Did the highest score of logit is equivalent to the actual value(returns booleans)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # We want the mean of the loss for every instance.\n    \n\n\n# Init and Saver\ninit = tf.global_variables_initializer() # This variable initializes all variables.\nsaver = tf.train.Saver() # Saves the training set parameters.","execution_count":48,"outputs":[]},{"metadata":{"_cell_guid":"10f14fda-3f24-4f15-9345-38909e8a0aec","_uuid":"f448ef995f612b6e462abdbab3a056384b0f6934","trusted":true},"cell_type":"code","source":"amnt_epochs = 5\nbatch_size = 100\n\nwith tf.Session() as sess:\n    init.run()\n    \n    for epoch in range(amnt_epochs):\n        epoch_loss = 0\n        i=0\n        while i < len(X_train):\n            start = i\n            end = i+batch_size\n            batch_x = np.array(X_train[start:end])\n            batch_y = np.array(y_train[start:end])\n\n            _, c = sess.run([training_op, loss], feed_dict={X: batch_x,\n                                              y: batch_y})\n            epoch_loss += c\n            i+=batch_size\n        acc_train = accuracy.eval(feed_dict={X: batch_x, y: batch_y})\n        acc_test = accuracy.eval(feed_dict={X: X_test, y:y_test})\n\n        print(epoch+1, 'Train accuracy: ', acc_train, 'Test accuracy: ', acc_test, 'Loss: ', epoch_loss)","execution_count":49,"outputs":[]},{"metadata":{"_cell_guid":"3fe76f2a-6186-4434-b58e-7ff633f532f2","_uuid":"afddcbd5f4363d393a7471f613d14ec22205f0fa"},"cell_type":"markdown","source":"## Notice:\nThis is a quick exploratory analysis, which will be **constantly updated**. I will analyze new metrics and go deeper into what determines the level of risk of each customer. My plan for the moment, is to later use a classification algorithm to detect whether a loan is a good or bad, which will help us asses whether we should issue a loan or not when new information about a customer comes in. Nevertheless, this work is going to take me approximately several months so be patient with the updates in case you are interested with this project.\n\n## Tensorflow and the Dataset: \n**Note**: Remember that bad loans only make up about 7.2% of the whole dataset! So it is most likely that the training score will be high as well as the test score. Nevertheless, if someone can update the results of this dataset (I don't know if it exists) but remember, most of our loans are \"Current\" that means there is a risk that those loans might turn to bad loans. If there is a dataset that has the results after the term of all the loans were over that will be more useful in order for us to apply a practical Neural Network. Keep in mind this aspect of the dataset."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}