{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6d96fa08-6096-4546-b61a-f77c311e62b6"
      },
      "source": [
        "# Python for Padawans\n",
        "\n",
        "This tutorial will go throughthe basic data wrangling workflow I'm sure you all love to hate, in Python! \n",
        "FYI: I come from a R background (aka I'm not a proper programmer) so if you see any formatting issues please cut me a bit of slack. \n",
        "\n",
        "**The aim for this post is to show people how to easily move their R workflows to Python (especially pandas/scikit)**\n",
        "\n",
        "One thing I especially like is how consistent all the functions are. You don't need to switch up style like you have to when you move from base R to dplyr etc. \n",
        "|\n",
        "And also, it's apparently much easier to push code to production using Python than R. So there's that. \n",
        "\n",
        "### 1. Reading in libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cc55fa70-5b7b-4671-a99b-0b12f05c66c6"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8e0acc4f-f5f6-459a-9c52-593b304458fa"
      },
      "source": [
        "#### Don't forget that %matplotlib function. Otherwise your graphs will pop up in separate windows and stop the execution of further cells. And nobody got time for that.\n",
        "\n",
        "### 2. Reading in data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "914fa752-4895-460d-bd13-2c2170f17e20"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('D:/Mi-Data/Data/loan.csv', low_memory=False)\n",
        "data.drop(['id', 'member_id', 'emp_title'], axis=1, inplace=True)\n",
        "\n",
        "data.replace('n/a', np.nan,inplace=True)\n",
        "data.emp_length.fillna(value=0,inplace=True)\n",
        "\n",
        "data['emp_length'].replace(to_replace='[^0-9]+', value='', inplace=True, regex=True)\n",
        "data['emp_length'] = data['emp_length'].astype(int)\n",
        "\n",
        "data['term'] = data['term'].apply(lambda x: x.lstrip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1bcf60f7-24dc-4591-8804-d5791c864640"
      },
      "source": [
        "### 3. Basic plotting using Seaborn\n",
        "\n",
        "Now let's make some pretty graphs. Coming from R I definitely prefer ggplot2 but the more I use Seaborn, the more I like it. If you kinda forget about adding \"+\" to your graphs and instead use the dot operator, it does essentially the same stuff.\n",
        "\n",
        "**And I've just found out that you can create your own style sheets to make life easier. Wahoo!**\n",
        "\n",
        "But anyway, below I'll show you how to format a decent looking Seaborn graph, as well as how to summarise a given dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f95328c4-9b76-436d-b735-2fe9d2d30217"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib\n",
        "\n",
        "s = pd.value_counts(data['emp_length']).to_frame().reset_index()\n",
        "s.columns = ['type', 'count']\n",
        "\n",
        "def emp_dur_graph(graph_title):\n",
        "\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    ax = sns.barplot(y = \"count\", x = 'type', data=s)\n",
        "    ax.set(xlabel = '', ylabel = '', title = graph_title)\n",
        "    ax.get_yaxis().set_major_formatter(\n",
        "    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "    _ = ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
        "    \n",
        "emp_dur_graph('Distribution of employment length for issued loans')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e75175e5-59dd-428e-a8c4-7d501ca3315e"
      },
      "source": [
        "### 4. Using Seaborn stylesheets\n",
        "\n",
        "Now before we move on, we'll look at using style sheets to customize our graphs nice and quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d6b9ac7d-dd09-43d4-8d11-61a16ff9040c"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib\n",
        "\n",
        "print (plt.style.available)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8d588a58-ac89-417d-8352-35ba8bedbc90"
      },
      "source": [
        "Now you can see that we've got quite a few to play with. I'm going to focus on the following styles:\n",
        "\n",
        "- fivethirtyeight (because it's my fav website)\n",
        "- seaborn-notebook\n",
        "- ggplot\n",
        "- classic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e28dfade-d8ab-46f6-a7f6-e10647b84d2c"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "ax = emp_dur_graph('Fivethirty eight style')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0b0f33d7-4f18-46fc-9b4c-544ddfd7a8bd"
      },
      "outputs": [],
      "source": [
        "plt.style.use('seaborn-notebook')\n",
        "ax = emp_dur_graph('Seaborn-notebook style')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3219bdf4-4264-4e16-9647-73c52cafb2f4"
      },
      "outputs": [],
      "source": [
        "plt.style.use('ggplot')\n",
        "ax = emp_dur_graph('ggplot style')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9c390447-c9e0-47c0-8da8-3956ae12016c"
      },
      "outputs": [],
      "source": [
        "plt.style.use('classic')\n",
        "ax = emp_dur_graph('classic style')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "83e210f3-a4eb-4f26-9cbf-5dc6a1903f97"
      },
      "source": [
        "### 5. Working with dates\n",
        "\n",
        "Now we want to looking at datetimes. Dates can be quite difficult to manipulate but it's worth the wait. Once they're formatted correctly life becomes much easier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b8afabb4-ba30-4d19-959f-54d3598d0a4a"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "data.issue_d.fillna(value=np.nan,inplace=True)\n",
        "issue_d_todate = pd.to_datetime(data.issue_d)\n",
        "data.issue_d = pd.Series(data.issue_d).str.replace('-2015', '')\n",
        "data.emp_length.fillna(value=np.nan,inplace=True)\n",
        "\n",
        "data.drop(['loan_status'],1, inplace=True)\n",
        "\n",
        "data.drop(['pymnt_plan','url','desc','title' ],1, inplace=True)\n",
        "\n",
        "data.earliest_cr_line = pd.to_datetime(data.earliest_cr_line)\n",
        "import datetime as dt\n",
        "data['earliest_cr_line_year'] = data['earliest_cr_line'].dt.year"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9dc1d890-00e7-4e67-983a-991b4114fd87"
      },
      "source": [
        "### 6. Making faceted graphs using Seaborn\n",
        "\n",
        "Now I'll show you how you can build on the above data frame summaries as well as make some facet graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "39bcdff0-cd45-4e11-bb2c-4f7c8ae59e6f"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "s = pd.value_counts(data['earliest_cr_line']).to_frame().reset_index()\n",
        "s.columns = ['date', 'count']\n",
        "\n",
        "s['year'] = s['date'].dt.year\n",
        "s['month'] = s['date'].dt.month\n",
        "\n",
        "d = s[s['year'] > 2008]\n",
        "\n",
        "plt.rcParams.update(plt.rcParamsDefault)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "g = sns.FacetGrid(d, col=\"year\")\n",
        "g = g.map(sns.pointplot, \"month\", \"count\")\n",
        "g.set(xlabel = 'Month', ylabel = '')\n",
        "axes = plt.gca()\n",
        "_ = axes.set_ylim([0, d.year.max()])\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "df293ff3-2481-436a-8624-a19099147270"
      },
      "source": [
        "Now I want to show you how to easily drop columns that match a given pattern. Let's drop any column that includes \"mths\" in it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ce2a3fd6-b0cd-402f-baba-fb793059fce3"
      },
      "outputs": [],
      "source": [
        "mths = [s for s in data.columns.values if \"mths\" in s]\n",
        "mths\n",
        "\n",
        "data.drop(mths, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d8b6550d-016f-431a-b48c-830ab077e802"
      },
      "source": [
        "### 7. Using groupby to create summary graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "96548401-ddae-45cf-8248-403e2054dda6"
      },
      "outputs": [],
      "source": [
        "group = data.groupby('grade').agg([np.mean])\n",
        "loan_amt_mean = group['loan_amnt'].reset_index()\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "ax = sns.barplot(y = \"mean\", x = 'grade', data=loan_amt_mean)\n",
        "ax.set(xlabel = '', ylabel = '', title = 'Average amount loaned, by loan grade')\n",
        "ax.get_yaxis().set_major_formatter(\n",
        "matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a846d690-2231-478f-be4f-3fa6b2e89487"
      },
      "source": [
        "### 8. More advanced groupby statements visualised with faceted graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f619c8a0-c228-4954-96bf-b3440738cd17"
      },
      "outputs": [],
      "source": [
        "filtered  = data[data['earliest_cr_line_year'] > 2008]\n",
        "group = filtered.groupby(['grade', 'earliest_cr_line_year']).agg([np.mean])\n",
        "\n",
        "graph_df = group['int_rate'].reset_index()\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.suptitle('bold figure suptitle', fontsize=14, fontweight='bold')\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "g = sns.FacetGrid(graph_df, col=\"grade\", col_wrap = 2)\n",
        "g = g.map(sns.pointplot, \"earliest_cr_line_year\", \"mean\")\n",
        "g.set(xlabel = 'Year', ylabel = '')\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0, graph_df['mean'].max()])\n",
        "_ = plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fc0aa027-0ab4-48dc-9ec8-c800d20732a1"
      },
      "source": [
        "### 9. Treatment of missing values\n",
        "This section is a toughie because there really is no correct answer. A pure data science/mining approach would test each of the approaches here using a CV split and include the most accurate treatment in their modelling pipeline.\n",
        "Here I have included the code for the following treatments:\n",
        "\n",
        "- Mean imputation\n",
        "- Median imputation\n",
        "- Algorithmic imputation\n",
        "\n",
        "I spent a large amount of time looking at 3. because I couldn't find anyone else who has implemented it, so I built it myself. In R it's very easy to use supervised learning techniques to impute missing values for a given variable (as shown here: https://www.kaggle.com/mrisdal/shelter-animal-outcomes/quick-dirty-randomforest) but sadly I couldn't find it done in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fce8ae80-aa7a-454e-8331-4a2e05bf6a47"
      },
      "outputs": [],
      "source": [
        "#data['emp_length'].fillna(data['emp_length'].mean())\n",
        "#data['emp_length'].fillna(data['emp_length'].median())\n",
        "#data['emp_length'].fillna(data['earliest_cr_line_year'].median())\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf =  RandomForestClassifier(max_depth=5, n_estimators=100, max_features=1)\n",
        "\n",
        "data['emp_length'].replace(to_replace=0, value=np.nan, inplace=True, regex=True)\n",
        "\n",
        "cat_variables = ['term', 'purpose', 'grade']\n",
        "columns = ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'int_rate', 'grade', 'purpose', 'term']\n",
        "\n",
        "def impute_missing_algo(df, target, cat_vars, cols, algo):\n",
        "\n",
        "    y = pd.DataFrame(df[target])\n",
        "    X = df[cols].copy()\n",
        "    X.drop(cat_vars, axis=1, inplace=True)\n",
        "\n",
        "    cat_vars = pd.get_dummies(df[cat_vars])\n",
        "\n",
        "    X = pd.concat([X, cat_vars], axis = 1)\n",
        "\n",
        "    y['null'] = y[target].isnull()\n",
        "    y['null'] = y.loc[:, target].isnull()\n",
        "    X['null'] = y[target].isnull()\n",
        "\n",
        "    y_missing = y[y['null'] == True]\n",
        "    y_notmissing = y[y['null'] == False]\n",
        "    X_missing = X[X['null'] == True]\n",
        "    X_notmissing = X[X['null'] == False]\n",
        "\n",
        "    y_missing.loc[:, target] = ''\n",
        "\n",
        "    dfs = [y_missing, y_notmissing, X_missing, X_notmissing]\n",
        "    \n",
        "    for df in dfs:\n",
        "        df.drop('null', inplace = True, axis = 1)\n",
        "\n",
        "    y_missing = y_missing.values.ravel(order='C')\n",
        "    y_notmissing = y_notmissing.values.ravel(order='C')\n",
        "    X_missing = X_missing.as_matrix()\n",
        "    X_notmissing = X_notmissing.as_matrix()\n",
        "    \n",
        "    algo.fit(X_notmissing, y_notmissing)\n",
        "    y_missing = algo.predict(X_missing)\n",
        "\n",
        "    y.loc[(y['null'] == True), target] = y_missing\n",
        "    y.loc[(y['null'] == False), target] = y_notmissing\n",
        "    \n",
        "    return(y[target])\n",
        "\n",
        "data['emp_length'] = impute_missing_algo(data, 'emp_length', cat_variables, columns, rf)\n",
        "data['earliest_cr_line_year'] = impute_missing_algo(data, 'earliest_cr_line_year', cat_variables, columns, rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e6888e90-dcb4-4bf9-b906-3cd9a4140af8"
      },
      "source": [
        "### 10. Running a simple classification model\n",
        "Here I take my cleaned variables (missing values have been imputed using random forests) and run a simple sklearn algo to classify the term of the loan.\n",
        "This step in the analytics pipeline does take longer in Python than in R (as R handles factor variables out of the box while sklearn only accepts numeric features) but it isn't that hard.\n",
        "This is just indicative though! A number of the variables are likely to introduce leakage to the prediction problem as they'll influence the term of the loan either directly or indirectly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9f19d4c7-6888-4e70-bf0f-56ab09140f36"
      },
      "outputs": [],
      "source": [
        "y = data.term\n",
        "\n",
        "cols = ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'int_rate', 'grade', 'emp_length', 'purpose', 'earliest_cr_line_year']\n",
        "X = pd.get_dummies(data[cols])\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "y = y.apply(lambda x: x.lstrip())\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(y)\n",
        "\n",
        "y = le.transform(y)\n",
        "X = X.as_matrix()\n",
        "\n",
        "from sklearn import linear_model\n",
        "\n",
        "logistic = linear_model.LogisticRegression()\n",
        "\n",
        "logistic.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1bc0952f-4dfc-45e4-aa08-a1fac8d4169d"
      },
      "source": [
        "### 11. Pipelining in sklearn\n",
        "\n",
        "In this section I'll go through how you can combine multiple techniques (supervised an unsupervised) in a pipeline.\n",
        "These can be useful for a number of reasons:\n",
        "\n",
        "- You can score the output of the whole pipeline\n",
        "- You can gridsearch for the whole pipeline making finding optimal parameters easier\n",
        "\n",
        "So next we'll combine some a PCA (unsupervised) and Random Forests (supervised) to create a pipeline for modelling the data. \n",
        "\n",
        "In addition to this I'll show you an easy way to grid search for the optimal hyper parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8740a86a-ac24-4514-a16f-4ff75ec17496"
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model, decomposition\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.grid_search import GridSearchCV\n",
        "\n",
        "rf = RandomForestClassifier(max_depth=5, max_features=1)\n",
        "\n",
        "pca = decomposition.PCA()\n",
        "pipe = Pipeline(steps=[('pca', pca), ('rf', rf)])\n",
        "\n",
        "n_comp = [3, 5]\n",
        "n_est = [10, 20]\n",
        "\n",
        "estimator = GridSearchCV(pipe,\n",
        "                         dict(pca__n_components=n_comp,\n",
        "                              rf__n_estimators=n_est))\n",
        "\n",
        "estimator.fit(X, y)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}